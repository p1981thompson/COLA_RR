---
title: "COLA Registered Report: Sample size justification"
date: "4th September 2020"
output: bookdown::word_document2
bibliography: power.bib
---

Scripts and rationale for power analysis: Dorothy Bishop and Paul Thompson.  


```{r Rpackage_setup, include=FALSE}
#need to install semPower from github on first run
knitr::opts_chunk$set(echo = TRUE)
library("devtools")
#install_github("moshagen/semPower")
library(semPower)
library(bookdown)
library(lavaan)
library(semPlot)
# library(knitr)
# library(kableExtra)
library(tidyverse)
library(flextable)
library(officer)
library(corrr) #easy correlations
library(pwr) #power analysis for common stats tests
library(MASS) #multivariate normal simulation
library(qpcR) #used in Kievit script
options(scipen=999)
```
<!---started by DVMB 10th August 2020 when new RR document created on Google docs.
NB output and bibliography statements from Paul. Updated multigroup models part by Paul on 19th August, and 3rd Sept--->
# Registered report: Inconsistent language lateralisation: evidence from behaviour and lateralised cerebral blood flow
 
## The COLA Consortium
 
<!--- See Registered Report_COLA20200808, on OneDrive/Oscci mss 2016-/ongoing/COLA RR for main intro. This corresponds to Google docs manuscripts and will change, so for now we just focus on Hypotheses/Predictions so that we can simulate a dataset for analysis and do power analysis --->

## Hypotheses

The overarching hypothesis is that there are cross-hemispheric dissociations in lateralisation of different language functions, especially in left handers.  A subsidiary hypothesis, derived from Woodhead et al (2019, 2020), is that laterality indices from tasks that primarily implicate language generation and receptive language will form separate clusters.
 
### Predictions

_Online behavioural measures_  

1. On online language tasks that show left-lateralisation for language, lateralisation will be stronger in right than left handers, in terms of central tendency as well as the proportion of participants who show the typical side bias (cf. Karlsson, Johnstone & Carey, 2019).

2. The pattern of correlation between laterality indices from online measures will reflect the extent to which they involve implicit speech production, rather than whether they involve spoken or written language. Thus we anticipate dissociation between the rhyme decision task, which involves implicit speech generation, and the word comprehension task and dichotic listening tasks, which do not. We further anticipate that dissociations between tasks are not accountable for in terms of low reliability of measures – i.e. correlations of laterality indices between tasks will be lower than split-half reliability of the measures.

_FTCD measures_  

3. The data will fit a model where 'language generation' tasks cluster together on one factor, and 'receptive' language tasks on a second factor.  The factors will be correlated, but the fit of a 2-factor model will be superior to a single-factor model.  

4. Following Woodhead et al (2020), we predict that better model fit will be obtained when different parameters are estimated for left- vs right handers, compared with when all parameters are held constant.

5. On categorical analysis, individuals who depart from left-brained laterality on one or more tasks will be more likely to be left-handed than those who are consistently 
left-lateralised. 
 
_Relationship between fTCD and behavioural laterality indices_  

6. The laterality profile obtained with the online language battery will be significantly associated with the profile seen with direct measure of cerebral blood flow using fTCD, with laterality on dichotic listening relating more strongly to receptive language tasks, and visual-half-field language measures to language generation tasks.   

# Sample size justification

The sample size is determined not solely by statistical power, though we aim for 90% power for key analyses. In addition, we take into consideration the fact that this is a two-stage study, with online testing followed by in-person testing. Recruitment for online testing is considerably easier and less costly than in-person testing. The timing of in-person testing remains uncertain due to pandemic restrictions, and this means that we may see considerable attrition between stages 1 and 2. Our strategy is first to determine sample size based on power considerations for stage 2, then to double that sample size for online testing, assuming there will be 50% attrition between stages 1 and 2. In addition, we check the power that this gives us for stage 1 testing. Accordingly, we report power analyses in reverse order for stage 2 and then stage 1.


## Single group confirmatory factor analysis models (one vs two factor)

Sample size determination has two aspects: 1. We need to have sufficient sample size to fit individual models with acceptable fit indices, regardless of whether these are one factor or two factor; 2. There should be adequate power to detect the misspecified model (one factor) vs the ‘true’ model (two factor). For the first point, we use the Monte Carlo simulation results for Confirmatory Factor Analysis (CFA) presented by Wolf et al (2013) @Wolf_2013. These authors reported results from a range of simple CFA models, varying numbers of factors, numbers of measured variables, and magnitudes of factor loadings, in order to evaluate statistical power, bias in parameter estimates and incidence of model convergence problems. Our proposed model comes closest to their simulation of a 2-factor model with 3 indicators per factor; the minimum sample size required to achieve 80% power depends on the strength of factor loadings, ranging between 120-200 for loadings of .65 or .8 respectively. This is in line with our next analysis, which estimates power to detect the difference between a one- and two-factor model.


### Power estimate for the model comparison of proposed 'true' model vs proposed mispecified model.

The `R` package `semPower` [@Moshagen_2016] can be used to calculate the power for detecting a difference between nested models, one factor vs two factors, using data simulated from both models. Our model comparison is closely similar to that used in the vignette for the semPower package by @Moshagen_2020. Moshagen (2020) provides one example where the interest is in whether responses on 8 items reflect two separate but correlated factors, or whether they can be described by a single factor; this is directly parallel to our prediction 3.  

Moshagen (2020) provides a further example that illustrates a power test for invariance constraints in a multiple group model, where the question is whether a model with freely estimated loadings performs better than a model where loadings are equal for two groups: this addresses our prediction 4.  

The starting point for both examples is a two-group model. Here we use scripts from these two examples with modifications to adapt the code for characteristics of our data. 

<!-- ![2 factor model](/Users/dorothybishop/deevybee_repo/COLA_RR_phase1_2/factorpic.jpg) -->
We first need to define a suitable model to simulate the true situation in the population. Here we use relevant data from our previous study (Woodhead et al, 2019, 2020) to guide our estimates. This had data from six measures, four of which we plan to use (in slightly modified form) in the current study, two loading on each of the postulated factors. These are Sentence Generation, Phonological Decision, Syntactic Decision and Sentence Comprehension; we term these x1, x2, x4 and x5. The additional two measures, x3 and x6, are selected to act as additional indicators of the language generation and receptive language factors respectively. To simulate data for x1, x2, x4 and x5 we use Session 1 laterality indices (LIs) from the Woodhead et al data, and to simulate x3 we take the mean from Session 2 for x1 and x2; to simulate x6 we take the mean from x4 and x5 from Session 2. This generates a dataset, SimAll, with correlation matrix as shown in Table 1. 

  
```{r sim.online, echo = FALSE}
#base predictions on observed data from study A2
myA2 <- read.csv('https://osf.io/fr5na/download') #read from OSF stored file
#We will have 2 new measures, one for factor A and one for factor B
#To simulate data, we will assume that we can estimate these by taking mean of the other two variables loading on that factor from the 2nd session - i.e. the data are independent from session 1.
#simulate correlation matrix by taking Factor A:
#PhonDec1,SentGen1 and mean PhonDec2/SentGen2
#vs Factor B: SentComp1,Jabber1,and mean SentComp2/Jabber2
myA2$PhonSent2 <- (myA2$PhonDec2+myA2$SentGen2)/2
myA2$CompJabb2 <- (myA2$SentComp2+myA2$Jabber2)/2 #(was wrongly specified as SentGen2 in original script)
corrcolumns <- c(3,5,33,6,7,34)


SimAll<-na.omit(myA2[,c(32,3,5,33,6,7,34)]) #just handedness and the 6 columns of interest retained
colnames(SimAll)<- c('hand','x1','x2','x3','x4','x5','x6')
mycortab <- correlate(SimAll[,2:7]) #check that correlation pattern looks realistic
ft<-flextable(mycortab)
ft <- colformat_num(x = ft,digits = 2)
ft <-set_caption(x=ft,"Table 1. Correlations: simulated population model")
ft

#Recording the means and sds by handedness here - helps interpretation later
#Left handers have larger sd for all
myagmean <- aggregate(SimAll[,2:7],by=list(SimAll$hand),FUN=mean)
myagsd <- aggregate(SimAll[,2:7],by=list(SimAll$hand),FUN=sd)
```

We next need to specify loadings for a population model compatible with our simulated data. Here we first derive loadings from fitting the simulated (SimAll) data.
```{r getloadings}
model.2f <- '
f1 =~ x1 + x2  + x3 
f2 =~ x4 + x5  +x6 
'
fit.mod2f <- cfa(model.2f,data=SimAll)
summary(fit.mod2f, fit.measures=TRUE)
standardizedSolution(fit.mod2f)
mys<-standardizedSolution(fit.mod2f)
```
We defined a 'true' two-factor population model, H0, using these factor loadings. This will be compared with an alternative, H1, single-factor model. In effect, we ask whether these two models could be differentiated on the basis of the covariance matrix from the simulated data.


```{r makepopmodel}
# define population model (= H0)
# Creating the formula using paste is rather confusing, but it does ensure we plug the correct loadings in to the model
model.pop<-paste0('f1 =~  ',mys[1,4],'*x1 + ',
                  mys[2,4],'* x2 + ',
                  mys[3,4],'* x3  
                  f2 =~ ', mys[4,4],'* x4 + ',
                  mys[5,4],'* x5 + ',
                  mys[6,4],'* x6  
                  f1~~ 1 * f1         
                  f2~~ 1 * f2 
                  f1~~', mys[15,4],'*f2')

# define (wrong) H1 model - f1 and f2 perfectly correlated, ie one factor
model.h1 <- '
f1 =~ x1 + x2 + x3
f2 =~ x4 + x5 + x6
f1~~ 1 * f1           # variance of f1 is 1
f2 ~~ 1*f2
f1 ~~ 1*f2

' 
```
Now we make a predicted covariance matrix for the population model and the alternative 1-factor model.

```{r popcov}

# get population covariance matrix; equivalent to a perfectly fitting H0 model
cov.h0 <- fitted(sem(model.pop))$cov
# get covariance matrix as implied by H1 model
res.h1 <- sem(model.h1, sample.cov = cov.h0, sample.nobs = 1000, likelihood='wishart')
df <- res.h1@test[[1]]$df
cov.h1 <- fitted(res.h1)$cov
```

```{r dopower}
# perform power analysis specifying 90% power
ap <- semPower.aPriori(SigmaHat = cov.h1, Sigma = cov.h0, alpha = .05, power = .90, df = df)
#summary(ap) #displays all content

```

This analysis estimates that for 90% power to detect superiority of the two-factor over the one-factor model, given true population correlations as in Table 1, we require a minimum sample size of `r ap$requiredN`.


## Multigroup model (two factor multigroup CFA - left and right handers groups)

This analysis is based on the second extended example in the semPower manual.  
We fit our two-factor model to two groups (left- and right-handers) and are interested in whether the same model estimates can be applied to both groups, or whether fit is better when this invariance requirement is relaxed. There are several potential ways in which the models could differ. On the basis of prior research, we anticipate mean differences in factor scores between left- and right-handers; the question of interest is whether these are sufficient to account for different patterns of laterality. Figure 5 illustrates how a mean shift alone would lead to more cases of opposite laterality in left-handers, even if all factor loadings and covariances in a model were the same. 
```{r demomeanshift}
jpeg('Fig5_scatters.jpg',width = 700, height = 300)
par(mfrow=c(1,2))
require(MASS)
for (gp in 1:2){ #2 groups, we'll set their means to be 1,1 and 2,2 for simplicity
mym <- c(gp,gp)
mysigma <- matrix(c(1,.5,.5,1),nrow=2)
myN <- 500
gp1 <- data.frame(mvrnorm(200,mym,mysigma))
plot(gp1,xlim=c(-2,5),ylim=c(-2,5),pch=16,cex=.7,xlab='LI Factor 1',ylab='LI Factor 2',main=paste0('Group mean = ',gp))
abline(h=0)
abline(v=0)
text(-1,4.8,'A',font=2)
text(3,4.8,'B',font=2)
text(-1,-1.8,'C',font=2)
text(3,-1.8,'D',font=2)
w1=which(gp1$X1<0)
w2=which(gp1$X2<0)
w3=which(gp1$X1>0)
w4=which(gp1$X2>0)
quadA <- length(intersect(w1,w4))
quadB <- length(intersect(w3,w4))
quadC <- length(intersect(w1,w2))
quadD <- length(intersect(w2,w3))
#Percentage in quads A, C and D 
gp2.notLL <- 100*(quadA+quadC+quadD)/myN
if(gp==1){gp1.notLL <- 100*(quadA+quadC+quadD)/myN}

}
dev.off()
```

Here we compared nested models that restrict parameters iteratively, to test for measurement invariance and then structural invariance between the groups, with mean differences tested at the final step. At each step, if the fit of the latter model is not significantly worse, this supports the conclusion that we can assume that level of measurement invariance.

We first specify a population model for each of the two groups. As before, we will use the SimAll data to derive estimated loadings, this time separately for each handedness group.

```{r leftrightcovs}
cov.sampleR<-cov(SimAll[SimAll$hand=='R',2:7])
cov.sampleL<-cov(SimAll[SimAll$hand=='L',2:7])
meansR <- colMeans(SimAll[SimAll$hand=='R',2:7])
meansL <- colMeans(SimAll[SimAll$hand=='L',2:7])

```


First we derive indices for a population model for each group by fitting the two factor model to data from our simulated population data, SimAll, this time differentiating by handedness group. We also specify 'meanstructure' to allow us to see how means differ between handedness groups.
```{r model2fac}

model.2f <- '
f1 =~ x1 + x2  + x3 
f2 =~ x4 + x5  + x6 
'

fit.mod2f <- cfa(model.2f,data=SimAll,group="hand", meanstructure=T)
summary(fit.mod2f, fit.measures=TRUE)

A6.z <- standardizedSolution(fit.mod2f) #try using parameters from standardized solution in population model

#We will wrangle this into a table to make it easier to compare groups.
A6.z$op <- paste(A6.z$lhs,A6.z$op,A6.z$rhs)
A6.z$lhs[1:23]<-c('path','path','path','path','path','path',
                        'variance','variance','variance','variance','variance','variance','variance','variance',
                        'covariance','mean','mean','mean','mean','mean','mean','mean','mean')

colnames(A6.z)[5:8]<-c('RH: estimate','RH: SE','LH: estimate','LH: SE')
A6.z[1:23,7:8]<-A6.z[24:46,5:6]
A6.z<-A6.z[1:23,c(1,2,5:8)]
A6.z[,3:6]<-round(A6.z[,3:6],3)
colnames(A6.z)[1:2]<-c('Type','Formula')
ft<-flextable(A6.z)
ft <-set_caption(x=ft,"Two-factor model: estimated loadings for right- and left-handers")
ft

```

We use the estimated factor loadings and covariance to specify separate population models for the two groups.  
```{r semPowerextended.eg}
#population model group 1 

model.pop.R<-paste0('f1 =~  ',A6.z[1,3],'*x1 + ',
                  A6.z[2,3],'* x2 + ',
                  A6.z[3,3],'* x3  
                  f2 =~ ', A6.z[4,3],'* x4 + ',
                  A6.z[5,3],'* x5 + ',
                  A6.z[6,3],'* x6  
                  x1 ~~ ',(1-A6.z[1,3]^2),'*x1
                  x2 ~~ ',(1-A6.z[2,3]^2),'*x2
                  x3 ~~ ',(1-A6.z[3,3]^2),'*x3
                  x4 ~~ ',(1-A6.z[4,3]^2),'*x4
                  x5 ~~ ',(1-A6.z[5,3]^2),'*x5
                  x6 ~~ ',(1-A6.z[6,3]^2),'*x6
                  f1~~ 1 * f1         
                  f2~~ 1 * f2 
                  f1~~', A6.z[15,3],'*f2')
#population model group 2 
model.pop.L<-paste0('f1 =~  ',A6.z[1,5],'*x1 + ',
                  A6.z[2,5],'* x2 + ',
                  A6.z[3,5],'* x3  
                  f2 =~ ', A6.z[4,5],'* x4 + ',
                  A6.z[5,5],'* x5 + ',
                  A6.z[6,5],'* x6  
                  x1 ~~ ',(1-A6.z[1,5]^2),'*x1
                  x2 ~~ ',(1-A6.z[2,5]^2),'*x2
                  x3 ~~ ',(1-A6.z[3,5]^2),'*x3
                  x4 ~~ ',(1-A6.z[4,5]^2),'*x4
                  x5 ~~ ',(1-A6.z[5,5]^2),'*x5
                  x6 ~~ ',(1-A6.z[6,5]^2),'*x6
                  f1~~ 1 * f1         
                  f2~~ 1 * f2 
                  f1~~', A6.z[15,5],'*f2')


```

Now we work out population covariances, assuming separate models for each group.
```{r groupLRcovs}
cov.pop.R <- fitted(sem(model.pop.R))$cov
cov.pop.L <- fitted(sem(model.pop.L))$cov

```

## Measurement Invariance

Configural invariance (sometimes known as pattern invariance) ensures that both groups follow the same pattern of factor loadings, i.e. same items load on to the same factors in both groups. Configural invariance is assumed in this study as this was previously established by Woodhead et al. (2020). We start by testing whether factor loadings are the same for each group (weak measurement invariance), which is a necessary prerequisite for interpreting later steps.

### Weak Measurement Invariance - same loadings (slopes) values for both groups.

Next we define and estimate the first analysis model, setting loadings to be equal - i.e. the alternative model (saturated model with no group constraints) tests whether the fit is as good when the simulated data are tested against a simpler model that has same loadings for both handedness groups.

```{r weak_measurement_invariance,echo=F}
model.hcommon <- '
f1 =~ x1+x2+x3
f2 =~ x4+x5+x6
'

# fit analysis model to population data; note the sample.nobs are arbitrary
fit.hcommon <- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000), group.equal = c('loadings','means'), likelihood='wishart',meanstructure=T,sample.cov.rescale=F)
# get model implied covariance matrices
cov.hcommon.R<- fitted(fit.hcommon)$`Group 1`$cov
cov.hcommon.L <- fitted(fit.hcommon)$`Group 2`$cov
# store df
df <- fit.hcommon@test[[1]]$df

fit.bl<- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000),group.equal=c('means'), likelihood='wishart',sample.cov.rescale=F)

df.bl<-fit.bl@test[[1]]$df

df.diff<-df-df.bl

```

We can now perform power-analysis for weak measurement invariance.  

```{r power_weak}
# perform a priori power analysis
# Note that lavaan handles multiple group models by having covariance matrices specified as lists containing the associated matrices for each group.  
ap6_weak <- semPower.aPriori(SigmaHat = list(cov.hcommon.R, cov.hcommon.L), 
                        Sigma = list(cov.pop.R, cov.pop.L), 
                        alpha = .05, power=.9, N=list(1,1),df = df.diff)

summary(ap6_weak)
#NB the N list gives ratio of L hander to R hander

```

The power analysis gives a minimum sample size of `r ap6_weak$requiredN`. Our intended sample size would not be sufficient to detect the different factor loadings from the simulated data. This is not of concern, as these differences, based on our prior data, are small enough to be disregarded. 



### Strong Measurement Invariance - same loadings (slopes) and item intercept and means values for both groups.

Next we define and estimate the second analysis model, setting loadings and item intercepts to be equal - i.e. the alternative model (loading and intercept constraints) tests whether the fit is as good when the simulated data are tested against a simpler model (loadings only) that has same loadings for both handedness groups. 

```{r strong_measurement_invariance1,echo=F}
model.hcommon <- '
f1 =~ x1+x2+x3
f2 =~ x4+x5+x6
'

# fit analysis model to population data; note the sample.nobs are arbitrary
fit.hcommon <- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000), group.equal = c('loadings',"intercepts"), likelihood='wishart',meanstructure=T,sample.cov.rescale=F)
# get model implied covariance matrices
cov.hcommon.R<- fitted(fit.hcommon)$`Group 1`$cov
cov.hcommon.L <- fitted(fit.hcommon)$`Group 2`$cov
# store df
df <- fit.hcommon@test[[1]]$df

fit.bl<- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000), group.equal = c('loadings','means'), likelihood='wishart',sample.cov.rescale=F)

df.bl<-fit.bl@test[[1]]$df

df.diff<-df-df.bl

```

We can now perform power-analysis for strong measurement invariance.  

```{r power_strong1}
# perform a priori power analysis
# Note that lavaan handles multiple group models by having covariance matrices specified as lists containing the associated matrices for each group.  
ap6_strong <- semPower.aPriori(SigmaHat = list(cov.hcommon.R, cov.hcommon.L), 
                        Sigma = list(cov.pop.R, cov.pop.L), 
                        alpha = .05, power=.9, N=list(1,1),df = df.diff)

summary(ap6_strong)

```

The power analysis for loadings and intercepts gives a minimum sample size of `r ap6_strong$requiredN`. As before, the analysis shows that any measurement invariance detected in the simulated data is small and would not be detected in our planned sample size. 


### Strict Measurement Invariance - same loadings, item intercepts and item means, and measurement error values for both groups.

Next we define and estimate the fourth analysis model, setting loadings, item intercepts and means, and measurement errors to be equal - i.e. the alternative model (loadings, item intercepts and means, and errors) tests whether the fit is as good when the simulated data are tested against a simpler model (unconstrained errors) that has same loadings for both handedness groups. This is usually only necessary to ensure that invariance of item reliabilities in both groups.

```{r strict_measurement_invariance,echo=F}
model.hcommon <- '
f1 =~ x1+x2+x3
f2 =~ x4+x5+x6
'

# fit analysis model to population data; note the sample.nobs are arbitrary
fit.hcommon <- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000), group.equal = c('loadings',"intercepts","residuals","residual.covariances"), likelihood='wishart',meanstructure=T,sample.cov.rescale=F)
# get model implied covariance matrices
cov.hcommon.R<- fitted(fit.hcommon)$`Group 1`$cov
cov.hcommon.L <- fitted(fit.hcommon)$`Group 2`$cov
# store df
df <- fit.hcommon@test[[1]]$df

fit.bl<- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000), group.equal = c('loadings','intercepts'), likelihood='wishart',sample.cov.rescale=F)

df.bl<-fit.bl@test[[1]]$df

df.diff<-df-df.bl

```

We can now perform power-analysis for strict measurement invariance.  

```{r power_strict}
# perform a priori power analysis
# Note that lavaan handles multiple group models by having covariance matrices specified as lists containing the associated matrices for each group.  
ap6_strict <- semPower.aPriori(SigmaHat = list(cov.hcommon.R, cov.hcommon.L), 
                        Sigma = list(cov.pop.R, cov.pop.L), 
                        alpha = .05, power=.9, N=list(1,1),df = df.diff)

summary(ap6_strict)

```

The power analysis gives a minimum sample size of `r ap6_strict$requiredN`. This suggests we may detect groups differences in residuals between handedness groups with our planned sample size, reflecting the fact that left-handers have higher variance than right-handers on some of the measures.

## Structural Invariance

### Factor Variance Invariance Model

Structural invariance is specific to the factors, testing whether firstly factor variances and covariances are invariant across groups, then factor means. We are interested in whether the factor covariances are different across groups so we only test this prediction here

```{r structural_invariance,echo=F}
model.hcommon <- '
f1 =~ x1+x2+x3
f2 =~ x4+x5+x6
'

# fit analysis model to population data; note the sample.nobs are arbitrary
fit.hcommon <- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000), group.equal = c('loadings',"intercepts","residuals","residual.covariances","lv.variances","lv.covariances"), likelihood='wishart',meanstructure=T,sample.cov.rescale=F)
# get model implied covariance matrices
cov.hcommon.R<- fitted(fit.hcommon)$`Group 1`$cov
cov.hcommon.L <- fitted(fit.hcommon)$`Group 2`$cov
# store df
df <- fit.hcommon@test[[1]]$df

fit.bl<- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000), group.equal = c('loadings','intercepts',"residuals","residual.covariances"), likelihood='wishart',sample.cov.rescale=F)

df.bl<-fit.bl@test[[1]]$df

df.diff<-df-df.bl

```

We can now perform power-analysis for structural invariance.  

```{r power_structural}
# perform a priori power analysis
# Note that lavaan handles multiple group models by having covariance matrices specified as lists containing the associated matrices for each group.  
ap6_structural <- semPower.aPriori(SigmaHat = list(cov.hcommon.R, cov.hcommon.L), 
                        Sigma = list(cov.pop.R, cov.pop.L), 
                        alpha = .05, power=.9, N=list(1,1),df = df.diff)

summary(ap6_structural)

```



### Factor Mean Invariance Model

Factor mean invariance is specific to the factor means, assuming firstly that factor variances and covariances are invariant across groups. We are interested in whether the factor means are different across groups.

```{r structural_invariance_means,echo=F}
model.hcommon <- '
f1 =~ x1+x2+x3
f2 =~ x4+x5+x6
'

# fit analysis model to population data; note the sample.nobs are arbitrary
fit.hcommon <- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000), group.equal = c('loadings',"intercepts","residuals","residual.covariances","lv.variances","lv.covariances","means"), likelihood='wishart',meanstructure=T,sample.cov.rescale=F)
# get model implied covariance matrices
cov.hcommon.R<- fitted(fit.hcommon)$`Group 1`$cov
cov.hcommon.L <- fitted(fit.hcommon)$`Group 2`$cov
# store df
df <- fit.hcommon@test[[1]]$df

fit.bl<- sem(model.hcommon, sample.cov = list(cov.pop.R, cov.pop.L), sample.nobs = list(1000, 1000), group.equal = c('loadings','intercepts',"residuals","residual.covariances","lv.variances","lv.covariances"), likelihood='wishart',sample.cov.rescale=F)

df.bl<-fit.bl@test[[1]]$df

df.diff<-df-df.bl

```

We can now perform power-analysis for structural invariance.  

```{r power_structural_means}
# perform a priori power analysis
# Note that lavaan handles multiple group models by having covariance matrices specified as lists containing the associated matrices for each group.  
ap6_structural_means <- semPower.aPriori(SigmaHat = list(cov.hcommon.R, cov.hcommon.L), 
                        Sigma = list(cov.pop.R, cov.pop.L), 
                        alpha = .05, power=.9, N=list(1,1),df = df.diff)

summary(ap6_structural_means)

```

The power analysis gives a minimum sample size of `r ap6_structural_means$requiredN`. 

```{r pwer_table}

powertab<-data.frame(Test=c('Weak Invariance','Strong Invariance','Strict Invariance','Structural factor variance Invariance','Structural factor mean Invariance'),N=c(ap6_weak$requiredN,ap6_strong$requiredN,ap6_strict$requiredN,ap6_structural$requiredN,ap6_structural_means$requiredN))

ft<-flextable(powertab)
ft <- colformat_num(x = ft,digits = 0)
ft <- autofit(ft)
ft <-set_caption(x=ft,"Sample size required for 90% power")
ft

```

### Prediction 5
Prediction 5 concerns categorical analysis. A simple approach is to dichotomise laterality at a cutoff of zero for each task, and then perform a chi square analysis to test for association with handedness.  For 6 measures, we adopt a Bonferroni-corrected alpha level of .05/6 = .008.

We can first examine sensitivity of this approach with the original sample of Woodhead et al (2020). 

```{r chisq.cats, include=F, echo=F, warning=F}
#initialise catx variables: 1 for +ve and - for negative
SimAll$catx1<-1
SimAll$catx2<-1
SimAll$catx3<-1
SimAll$catx4<-1
SimAll$catx5<-1
SimAll$catx6<-1

chitab<-data.frame(matrix(NA,ncol=11,nrow=6))
for (mycol in 1: 6){
  w<-which(SimAll[,(mycol+1)]<0) #find values below zero
  SimAll[w,(mycol+7)]<-0 #recode catx value to zero
  t<-table(SimAll[,(mycol+7)],SimAll$hand)
  tp<-round(prop.table(t,2),2) #proportions by handedness
  chitab[mycol,1]<-colnames(SimAll)[(mycol+1)]
  chitab[mycol,c(2,4,6,8)]<-t
  chitab[mycol,c(3,5,7,9)]<-tp
  chitab[mycol,10] <- chisq.test(t)$statistic
  chitab[mycol,11] <- chisq.test(t)$p.value
}
colnames(chitab)<-c('Measure','Lhand.Rbrain','LH.Rb%','Lhand.Lbrain','LH.Lb%','Rhand.Rbrain','RH.Rb%','Rhand.Lbrain','RH.Lb%','chisq','p')

```
This analysis shows that for the language production tasks, the proportions of left- and right-handers who are left-brain lateralised on the language production tasks are close to the proportions that have been obtained with other methods, such as Wada testing: around 90% for right-handers and around 60-70% for left-handers. For the receptive tasks, rates of left-brain bias decrease in both handedness groups, but a handedness effect persists.  

Given these results, we aim to achieve a sample size sufficient to detect a difference in left lateralisation of 70% in left-handers vs 90% in right-handers.  This difference in percentages translates to an effect size, h, of `r 2*asin(sqrt(.9))-2*asin(sqrt(.7))`. Using the R function `pwr.2p.test` we find that for power = .9 and alpha = .05, minimum N = `r round(pwr.2p.test(h=2*asin(sqrt(.9))-2*asin(sqrt(.7)),sig.level=.05,power=.9)$n,2)`.  Thus with the sample size of 100 per handedness group, we would be well-powered to detect this difference.  

## Sample size for online test battery
To allow for attrition of participants, we plan to recruit twice as many participants for online testing as we plan to test in person, namely 200 left-handers and 200 right-handers. For testing prediction 1, this will give us 90% power to detect handedness differences in LIs of `r round(pwr.t2n.test(n1 = 200, n2=200 , sig.level = .05, power = .9,alternative='greater')$d,2) ` on one-tailed t-test. 

For prediction 2 we focus on predicting patterns of covariance between the three online tasks, as we do not have sufficient indicators for modeling with latent variables. We prespecify four possible covariance structures, which are then compared using AIC weights. This is a subset of SEM that does not include any latent variables or directional paths. It allows us to constrain particular covariance patterns and report the 'best' model according AIC weights. 

The four models include:  
Model A, where all LIs are intercorrelated to a similar degree
Model B1,  where LIs for the two receptive language measures (dichotic listening and optimal viewing task) are intercorrelated, but independent of rhyme detection
Model B2, where LIs for the two tasks involving visual presentation and written language (optimal viewing and rhyme detection) are intercorrelated, and independent of the auditory task, dichotic listening.
Model C, where all LIs are independent of one another.

Models B1 and B2 are mathematically equivalent, differing only in the specific variables that are correlated. On a priori grounds, we favour model B1, which is compatible with our overall 2-factor model of language lateralisation. In order to compare models, we need to simulate data corresponding to each model type, and then evaluate how frequently the correct model is identified with our planned sample size of 400 participants.

```{r sim_dat}
#========================================================================#
# simulate multivariate normal data to test ESEM model on three outcomes.#
#========================================================================#
#Although we have 4 models, 2 (C and D) are computationally equivalent.
#In terms of power they will be the same, even though we are making predictions about different sets of tests.
sim_data<-function(n=400,r=.5)
  #we assume tasks are ovp,dichotic, and rhyme
{
  
  #========================================================================#
  sigA <- matrix(c(1,	r,r,
                   r, 1, r,
                   r, r, 1),3,3,byrow=TRUE)
  data_A <- mvrnorm(n=n, mu = c(0,0,0), Sigma=sigA) #all correlated
  #========================================================================#
  sigB <- matrix(c(1,	r,	0,
                   r, 1, 0,
                   0, 0, 1),3,3,byrow=TRUE)
  data_B <- mvrnorm(n=n, mu = c(0,0,0), Sigma=sigB)
  #ovp and dichotic correlated, rhyme independent
  #========================================================================#
  sigC <- matrix(c(1,	0,	0,
                   0, 1, 0,
                   0, 0, 1),3,3,byrow=TRUE)
  data_C <- mvrnorm(n=n, mu = c(0,0,0), Sigma=sigC) #all independent
  #========================================================================#
  
  
  data_A<-as.data.frame(data_A)
  data_B<-as.data.frame(data_B)
  data_C<-as.data.frame(data_C)
  
  names(data_A)<-names(data_B)<-names(data_C)<-c("rhyme","dichotic","ovp")
  
  return(list(data_A=data_A,data_B=data_B,data_C=data_C))
}
#========================================================================#
```

```{r aicmodels}
#Specify models

  #Model A all correlated
    modelA<-"
    rhyme~~dichotic
    rhyme~~ovp
    dichotic~~ovp
    "
    #Model B 2 correlated
    modelB<-"
    rhyme~~dichotic
    rhyme~~0*ovp
    dichotic~~0*ovp"
    
    #model C all independent
    modelC<-"
    rhyme~~0*dichotic
    rhyme~~0*ovp
    dichotic~~0*ovp"

    modelBB<-"
    rhyme~~0*dichotic
    rhyme~~ovp
    dichotic~~0*ovp"
    
```    

```{r makedata, warning = F}
myN <- 400 #sample size
myr <- .5 #correlation for intercorrelated variables
mydat<-sim_data(myN,myr)  #simulates sets of data for all 3 models
domakedata <-0
niter<-1000
if (domakedata==1){ #this is slow - can skip if you have already save AICdf


AICdf <- data.frame(matrix(NA,nrow=niter,ncol=14))
colnames(AICdf)<-c('modelA.dataA','modelB.dataA','modelC.dataA',
                   'modelA.dataB','modelB.dataB','modelC.dataB',
                   'modelA.dataC','modelB.dataC','modelC.dataC',
                   'bestfit.dataA','bestfit.dataB','bestfit.dataC',
                   'dataB.sigBvsC','modelBB.dataB')
  
  for(i in 1:niter)
  {
    sims <- sim_data(n=myN)
     #Using data simulated from model A (all correlated), test all models
    fit1A <- cfa(modelA, data=sims$data_A) #model has zero DF!
    fit2A <- cfa(modelB, data=sims$data_A)
    fit3A <- cfa(modelC, data=sims$data_A)
    
   aic_vector1<- c(fitMeasures(fit1A, "aic"),fitMeasures(fit2A, "aic"),fitMeasures(fit3A, "aic"))
    
    #Using data simulated from 2-correlated model, test all models
   #We also test model BB, where 2 are correlated, but the wrong 2
    fit1B <- cfa(modelA, data=sims$data_B)#model has zero DF!
    fit2B <- cfa(modelB, data=sims$data_B)
    fit3B <- cfa(modelC, data=sims$data_B)
    fit4BB <- cfa(modelBB,data=sims$data_B) 
    
    aic_vector2 <- c(fitMeasures(fit1B, "aic"),fitMeasures(fit2B, "aic"),fitMeasures(fit3B, "aic"))
    
    #Using data simulated from  model C (all uncorrelated), test all  models
    fit1C <- cfa(modelA, data=sims$data_C)#model has zero DF!
    fit2C <- cfa(modelB, data=sims$data_C)
    fit3C <- cfa(modelC, data=sims$data_C)
    
    aic_vector3 <- c(fitMeasures(fit1C, "aic"),fitMeasures(fit2C, "aic"),fitMeasures(fit3C, "aic"))
    
    aic_vector4 <-c(fitMeasures(fit4BB,'aic'),fitMeasures(fit2B, "aic"))
    
    #Anticipate that for each model, lowest AIC obtained when the simulated data do match the model
    # 
    # The AIC weights make massive difference for preferred model
    AICdf[i,1:3]<-akaike.weights(aic_vector1)$weights
     AICdf[i,4:6]<-akaike.weights(aic_vector2)$weights
    AICdf[i,7:9]<-akaike.weights(aic_vector3)$weights
    AICdf$bestfit.dataA[i] <- which(AICdf[i,1:3]==max(AICdf[i,1:3]))
    AICdf$bestfit.dataB[i] <- which(AICdf[i,4:6]==max(AICdf[i,4:6]))
    AICdf$bestfit.dataC[i] <- which(AICdf[i,7:9]==max(AICdf[i,7:9]))
    
#If true model is model B, can it be distinguished from model C (all independent)
    AICdf$dataB.sigBvsC[i] <- anova(fit2B,fit3B)$`Pr(>Chisq)`[2]
    
   BvsBB<-akaike.weights(aic_vector4)$weights[1] #comparison of true vs wrong assignment to correlations for model B; if this is < .5, then true model is better.
     AICdf[i,14]<-ifelse(BvsBB<.5,1,0)
}
}
rr<-round(myr*100,0)
AICname<-paste0('AICdf_r',rr,'niter',niter,'.csv')
if (domakedata==0){AICdf<-read.csv(AICname)}
myt1<-table(AICdf$bestfit.dataA)
myt2<-table(AICdf$bestfit.dataB)
myt3<-table(AICdf$bestfit.dataC)
myt4<-table(AICdf$modelBB.dataB)
powerBC <-length(which(AICdf$dataB.sigBvsC<.05))/niter

if(domakedata==1)
  {write.csv(AICdf,AICname,row.names=F)}

```

On 1000 runs, data for 400 participants were simulated to correspond to model B1, with a correlation of .5 between rhyme judgement and OVP, and zero correlation of these LIs with dichotic listening. On `r (100*myt2[2]/niter)`% of runs, model B1 had the highest value of Akaike weights, with model A (all tests correlated) on  the other `r (100*myt2[1]/niter)`% of runs. On `r 100*powerBC` % of runs, the correct model was significantly superior to model C (zero correlation between LIs), as evidenced by a significant chi square value on likelihood ratio test between models. Model B1 always had higher Akaike weights than model B2, when the data were generated from model B1. On the other hand, if data were generated from model C, ie. a true null model, with zero correlation between the three LIs, then model B1 gave highest Akaike weights on `r (100*myt3[2]/niter)`% of runs, and model A (all correlated) gave highest Akaike weights on `r (100*myt3[1]/niter)`%  of runs. <!---warning: there might be circumstances where this lookup of power values could be out . It depends on myt3 having values for all 3 models--->












# References